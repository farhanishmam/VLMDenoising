{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from glob import glob\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Folder Images count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base directory where noisy image folders are located\n",
    "base_dir = 'Data/Images/'\n",
    "\n",
    "\n",
    "# Iterate through each subfolder in the root directory\n",
    "for folder_name in os.listdir(base_dir):\n",
    "    folder_path = os.path.join(base_dir, folder_name)\n",
    "    \n",
    "    # Check if it's a directory\n",
    "    if os.path.isdir(folder_path):\n",
    "        # Count the number of files in the directory\n",
    "        file_count = len([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])\n",
    "        print(f\"{folder_name}: {file_count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def organize_dataset(clean_dir, noisy_base_dir, output_dir):\n",
    "#     \"\"\"\n",
    "#     Organize clean images and noisy images into the desired structure.\n",
    "    \n",
    "#     Args:\n",
    "#         clean_dir (str): Directory containing all clean images.\n",
    "#         noisy_base_dir (str): Directory containing noisy images in folders like Brightness_L1, Gaussian_blur_L2, etc.\n",
    "#         output_dir (str): Output directory where the organized dataset will be saved.\n",
    "#     \"\"\"\n",
    "#     # Ensure the output directories exist\n",
    "#     clean_output_dir = os.path.join(output_dir, 'clean')\n",
    "#     if not os.path.exists(clean_output_dir):\n",
    "#         os.makedirs(clean_output_dir)\n",
    "\n",
    "#     # Copy all clean images into the 'clean' folder\n",
    "#     clean_images = os.listdir(clean_dir)\n",
    "#     for img_name in clean_images:\n",
    "#         clean_img_path = os.path.join(clean_dir, img_name)\n",
    "#         shutil.copy(clean_img_path, os.path.join(clean_output_dir, img_name))\n",
    "\n",
    "#     # List all noisy subfolders\n",
    "#     noisy_folders = [f for f in os.listdir(noisy_base_dir) if os.path.isdir(os.path.join(noisy_base_dir, f))]\n",
    "\n",
    "#     # Organize noisy images by noise type and severity level\n",
    "#     for noisy_folder in noisy_folders:\n",
    "#         if '_L' in noisy_folder:  # Check if the folder name contains '_L'\n",
    "#             noise_type, severity = noisy_folder.split('_L')  # Splitting to get noise type and severity level\n",
    "#             noisy_folder_path = os.path.join(noisy_base_dir, noisy_folder)\n",
    "\n",
    "#             # Create output folder for this noise type and severity\n",
    "#             output_noise_type_dir = os.path.join(output_dir, noise_type)\n",
    "#             output_severity_dir = os.path.join(output_noise_type_dir, f\"L{severity}\")\n",
    "            \n",
    "#             if not os.path.exists(output_severity_dir):\n",
    "#                 os.makedirs(output_severity_dir)\n",
    "\n",
    "#             # Copy noisy images to the appropriate directory\n",
    "#             for img_name in os.listdir(noisy_folder_path):\n",
    "#                 noisy_img_path = os.path.join(noisy_folder_path, img_name)\n",
    "#                 shutil.copy(noisy_img_path, os.path.join(output_severity_dir, img_name))\n",
    "#         else:\n",
    "#             print(f\"Skipping folder '{noisy_folder}' as it does not contain a severity level.\")\n",
    "\n",
    "#     print(\"Dataset organized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directories\n",
    "# clean_dir = 'filtered_images/'\n",
    "# noisy_base_dir = 'Data/Images/'\n",
    "# output_dir ='Brightness/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noisy_folders = [f for f in os.listdir(noisy_base_dir) if os.path.isdir(os.path.join(noisy_base_dir, f))]\n",
    "# print(noisy_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize_dataset(clean_dir, noisy_base_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from PIL import Image\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dir = 'filtered_images/'\n",
    "noisy_base_dir = 'Saturation/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Clean and Noisy Image Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenoisingDataset(Dataset):\n",
    "    def __init__(self, clean_dir, noisy_base_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            clean_dir (string): Directory with all the clean images.\n",
    "            noisy_base_dir (string): Directory with subfolders for each noise type, and inside each noise type, folders for severity levels.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample (e.g., ToTensor, Normalize).\n",
    "        \"\"\"\n",
    "        self.clean_dir = clean_dir\n",
    "        self.noisy_base_dir = noisy_base_dir\n",
    "        self.transform = transform\n",
    "        self.clean_images = sorted(os.listdir(clean_dir))\n",
    "        # self.noise_types = sorted(os.listdir(noisy_base_dir))  # e.g., Brightness, Contrast\n",
    "        \n",
    "        self.noisy_images = []\n",
    "        # for noise_type in self.noise_types:\n",
    "        for severity_level in range(1, 6):  # Assuming L1 to L5\n",
    "            noisy_folder = os.path.join(noisy_base_dir, f'L{severity_level}')\n",
    "            print(noisy_folder)\n",
    "            noisy_images = sorted(os.listdir(noisy_folder))\n",
    "            print(noisy_images)\n",
    "            self.noisy_images.extend([(os.path.join(noisy_folder, img), os.path.join(clean_dir, img)) for img in noisy_images])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.noisy_images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        noisy_path, clean_path = self.noisy_images[idx]\n",
    "        noisy_image = Image.open(noisy_path).convert('RGB')\n",
    "        clean_image = Image.open(clean_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            noisy_image = self.transform(noisy_image)\n",
    "            clean_image = self.transform(clean_image)\n",
    "\n",
    "        return noisy_image, clean_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the dataset\n",
    "dataset = DenoisingDataset(clean_dir=clean_dir, \n",
    "                           noisy_base_dir=noisy_base_dir,\n",
    "                           transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the correct noisy-clean pairs are returned\n",
    "def test_denoising_dataset(dataset, num_samples=3001):\n",
    "    for i in range(num_samples):\n",
    "        noisy_image_path, clean_image_path = dataset.noisy_images[i]  # Get the file paths of noisy and clean images\n",
    "        \n",
    "        # Extract file names from the paths\n",
    "        noisy_folder_name = \"/\".join(noisy_image_path.split('/')[-3:-1])  # Folder name of noisy image\n",
    "        noisy_image_name = noisy_image_path.split('/')[-1]  # File name of noisy image\n",
    "        \n",
    "        clean_folder_name = clean_image_path.split('/')[-2]  # Folder name of clean image\n",
    "        clean_image_name = clean_image_path.split('/')[-1]  # File name of clean image\n",
    "        \n",
    "        # Print folder and file names to confirm matching\n",
    "        print(f\"Clean Image: Folder = {clean_folder_name}, Image = {clean_image_name}\")\n",
    "        print(f\"Noisy Image: Folder = {noisy_folder_name}, Image = {noisy_image_name}\\n\")\n",
    "\n",
    "# Test the dataset to see 5 random noisy-clean image file pairs\n",
    "test_denoising_dataset(dataset, num_samples=3001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deepinv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change the model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the BatchRenormalization Layer in PyTorch\n",
    "class BatchRenormalization(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-3):\n",
    "        super(BatchRenormalization, self).__init__()\n",
    "        self.bn = nn.BatchNorm2d(num_features, eps=eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.bn(x)\n",
    "\n",
    "# Define BRDNet Model\n",
    "class BRDNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BRDNet, self).__init__()\n",
    "        \n",
    "        # First set of layers for `x` branch\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.br1 = BatchRenormalization(64)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.conv_blocks_x = nn.Sequential(\n",
    "            *[nn.Sequential(\n",
    "                nn.Conv2d(64, 64, kernel_size=3, padding=1), \n",
    "                BatchRenormalization(64),\n",
    "                nn.ReLU()\n",
    "            ) for _ in range(7)]\n",
    "        )\n",
    "        \n",
    "        self.conv_blocks_y = nn.Sequential(\n",
    "            *[nn.Sequential(\n",
    "                nn.Conv2d(64, 64, kernel_size=3, padding=2, dilation=2), ## chamged padding to 2 from 1, added dilation\n",
    "                nn.ReLU()\n",
    "            ) for _ in range(7)]\n",
    "        )\n",
    "\n",
    "        # Last layers for `x`\n",
    "        self.conv2 = nn.Conv2d(64, 3, kernel_size=3, padding=1)\n",
    "\n",
    "        # First set of layers for `y` branch\n",
    "        self.conv_y_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.br_y_1 = BatchRenormalization(64)\n",
    "\n",
    "        # Additional layers for `y`\n",
    "        self.conv_blocks_y_extra = nn.Sequential(\n",
    "            *[nn.Sequential(\n",
    "                nn.Conv2d(64, 64, kernel_size=3, padding=2, dilation=2), ## chamged padding to 2 from 1\n",
    "                nn.ReLU()\n",
    "            ) for _ in range(6)]\n",
    "        )\n",
    "\n",
    "        # Final layers for `y`\n",
    "        self.conv_y_final = nn.Conv2d(64, 3, kernel_size=3, padding=1)\n",
    "\n",
    "        # Concatenation branch and final output\n",
    "        self.conv_concat = nn.Conv2d(6, 3, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First branch `x`\n",
    "        x_in = x\n",
    "        # print(f\"x_in shape: {x_in.shape}\")\n",
    "        x = self.relu(self.br1(self.conv1(x)))\n",
    "        # print(f\"x shape after conv1: {x.shape}\")\n",
    "        x = self.conv_blocks_x(x)\n",
    "        # print(f\"x shape after conv_blocks_x: {x.shape}\")\n",
    "        x = self.conv2(x)\n",
    "        # print(f\"x shape after conv2: {x.shape}\")\n",
    "        x = x_in - x  # input - noise\n",
    "\n",
    "        # Second branch `y`\n",
    "        y_in = x_in\n",
    "        # print(f\"y_in shape: {y_in.shape}\")\n",
    "        y = self.relu(self.br_y_1(self.conv_y_1(y_in)))\n",
    "        # print(f\"y shape after conv1: {y.shape}\")\n",
    "        y = self.conv_blocks_y(y)\n",
    "        # print(f\"y shape after conv_blocks_x: {y.shape}\")\n",
    "        y = self.conv_blocks_y_extra(y)\n",
    "        # print(f\"y shape after conv_blocks_y_extra: {y.shape}\")\n",
    "        y = self.conv_y_final(y)\n",
    "        # print(f\"y shape after conv_y_final: {y.shape}\")\n",
    "        y = y_in - y  # input - noise\n",
    "        # print(f\"y shape after y: {y.shape}\")\n",
    "\n",
    "        # Concatenation of `x` and `y`\n",
    "        out = torch.cat([x, y], dim=1)\n",
    "        out = self.conv_concat(out)\n",
    "\n",
    "        # Final output\n",
    "        out = x_in - out  # input - noise\n",
    "        return out\n",
    "\n",
    "# Instantiate the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class BatchRenormalization(nn.Module):\n",
    "#     def __init__(self, num_features, epsilon=1e-3):\n",
    "#         super(BatchRenormalization, self).__init__()\n",
    "#         self.bn = nn.BatchNorm2d(num_features, eps=epsilon)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         return self.bn(x)\n",
    "\n",
    "# class BRDNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(BRDNet, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n",
    "#         self.bn1 = BatchRenormalization(64)\n",
    "\n",
    "#         # First block: 7 layers of Conv + BN + ReLU\n",
    "#         self.conv_blocks_1 = nn.ModuleList([\n",
    "#             nn.Sequential(\n",
    "#                 nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "#                 BatchRenormalization(64),\n",
    "#                 nn.ReLU(inplace=True)\n",
    "#             ) for _ in range(7)\n",
    "#         ])\n",
    "\n",
    "#         # Last layers of first block: 8 layers of Conv + BN + ReLU\n",
    "#         self.conv_blocks_2 = nn.ModuleList([\n",
    "#             nn.Sequential(\n",
    "#                 nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "#                 BatchRenormalization(64),\n",
    "#                 nn.ReLU(inplace=True)\n",
    "#             ) for _ in range(8)\n",
    "#         ])\n",
    "\n",
    "#         self.final_conv_1 = nn.Conv2d(64, 3, kernel_size=3, padding=1)\n",
    "        \n",
    "#         # Second block\n",
    "#         self.conv2 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "#         self.bn2 = BatchRenormalization(64)\n",
    "\n",
    "#         # 7 layers of Conv + ReLU for the second block\n",
    "#         self.conv_blocks_3 = nn.ModuleList([\n",
    "#             nn.Sequential(\n",
    "#                 nn.Conv2d(64, 64, kernel_size=3, padding=1, dilation=2),\n",
    "#                 nn.ReLU(inplace=True)\n",
    "#             ) for _ in range(7)\n",
    "#         ])\n",
    "\n",
    "#         # Last layers of second block\n",
    "#         self.final_conv_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "#         self.bn3 = BatchRenormalization(64)\n",
    "        \n",
    "#         # 6 layers of Conv + ReLU for the second block\n",
    "#         self.conv_blocks_4 = nn.ModuleList([\n",
    "#             nn.Sequential(\n",
    "#                 nn.Conv2d(64, 64, kernel_size=3, padding=1, dilation=2),\n",
    "#                 nn.ReLU(inplace=True)\n",
    "#             ) for _ in range(6)\n",
    "#         ])\n",
    "\n",
    "#         self.final_conv_3 = nn.Conv2d(64, 3, kernel_size=3, padding=1)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # First block\n",
    "#         x1 = self.conv1(x)\n",
    "#         x1 = self.bn1(x1)\n",
    "#         x1 = F.relu(x1)\n",
    "        \n",
    "#         for layer in self.conv_blocks_1:\n",
    "#             x1 = layer(x1)\n",
    "\n",
    "#         for layer in self.conv_blocks_2:\n",
    "#             x1 = layer(x1)\n",
    "\n",
    "#         x1 = self.final_conv_1(x1)\n",
    "#         x1 = x - x1  # Subtract input from the output\n",
    "        \n",
    "#         # Second block\n",
    "#         y = self.conv2(x)\n",
    "#         y = self.bn2(y)\n",
    "#         y = F.relu(y)\n",
    "\n",
    "#         for layer in self.conv_blocks_3:\n",
    "#             y = layer(y)\n",
    "        \n",
    "#         y = self.final_conv_2(y)\n",
    "#         y = self.bn3(y)\n",
    "#         y = F.relu(y)\n",
    "\n",
    "#         for layer in self.conv_blocks_4:\n",
    "#             y = layer(y)\n",
    "\n",
    "#         y = self.final_conv_3(y)\n",
    "#         y = x - y  # Subtract input from the output\n",
    "\n",
    "#         # Concatenate outputs from both branches\n",
    "#         o = torch.cat((x1, y), dim=1)\n",
    "#         z = self.final_conv_1(o)\n",
    "#         z = x - z  # Final subtraction from input\n",
    "\n",
    "#         return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brd_model = BRDNet()\n",
    "brd_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from deepinv.models import DRUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DRUNet(in_channels=3, out_channels=3, pretrained='download', device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brd_model = brd_model.to(device)\n",
    "brd_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input = torch.randn(30, 3, 224, 224).to(device)\n",
    "with torch.no_grad():\n",
    "    output = brd_model(example_input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = len(dataset)\n",
    "total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.70 * total_size)  # 70% for training\n",
    "val_size = int(0.15 * total_size)    # 15% for validation\n",
    "test_size = total_size - train_size - val_size  # Remaining 15% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train, validation, and test sets\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "batch_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    # Convert tensor to numpy array, and unnormalize if needed\n",
    "    img = img.numpy().transpose((1, 2, 0))  # Change from CxHxW to HxWxC\n",
    "    img = np.clip(img, 0, 1)  # Ensure values are in [0, 1]\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "# Get a batch of training images\n",
    "data_iter = iter(train_loader)\n",
    "noisy_images, clean_images = next(data_iter)\n",
    "\n",
    "# Plot some images\n",
    "num_images = 4  # Number of images to display\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for i in range(num_images):\n",
    "    plt.subplot(2, num_images // 2, i + 1)\n",
    "    imshow(noisy_images[i])\n",
    "    plt.title('Noisy Image')\n",
    "    \n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for i in range(num_images):\n",
    "    plt.subplot(2, num_images // 2, i + 1)\n",
    "    imshow(clean_images[i])\n",
    "    plt.title('Clean Image')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print dataset sizes\n",
    "print(f\"Train dataset size: {len(train_dataset)} images\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)} images\")\n",
    "print(f\"Test dataset size: {len(test_dataset)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(outputs, clean_images):\n",
    "    \"\"\"Calculate PSNR and SSIM between outputs and clean images.\"\"\"\n",
    "    batch_size = outputs.size(0)\n",
    "    # print(batch_size)\n",
    "    psnr_values = []\n",
    "    ssim_values = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Reshape images to (224, 224, 3) if they are flattened\n",
    "        output_image = outputs[i].cpu().detach().numpy().reshape(224, 224, 3)\n",
    "        clean_image = clean_images[i].cpu().detach().numpy().reshape(224, 224, 3)\n",
    "\n",
    "        # Calculate PSNR\n",
    "        psnr_values.append(psnr(clean_image, output_image, data_range=clean_image.max() - clean_image.min())) # stores psnr values for each image in the batch\n",
    "        \n",
    "        # Calculate SSIM\n",
    "        ssim_values.append(ssim(clean_image, output_image, data_range=clean_image.max() - clean_image.min(), multichannel=True, win_size=3)) # stores ssim values for each image in the batch\n",
    "\n",
    "    return np.mean(psnr_values), np.mean(ssim_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_metrics(outputs, clean_images):\n",
    "#     \"\"\"Calculate PSNR and SSIM between outputs and clean images.\"\"\"\n",
    "#     psnr_values = []\n",
    "#     ssim_values = []\n",
    "\n",
    "#     # Iterate through each image in the batch\n",
    "#     for i in range(outputs.size(0)):  # Dynamically get batch size\n",
    "#         # Convert PyTorch tensors to NumPy arrays\n",
    "#         output_image = outputs[i].cpu().detach().numpy().transpose(1, 2, 0)  # (C, H, W) -> (H, W, C)\n",
    "#         clean_image = clean_images[i].cpu().detach().numpy().transpose(1, 2, 0)  # (C, H, W) -> (H, W, C)\n",
    "\n",
    "#         # Ensure pixel values are in the range [0, 1]\n",
    "#         output_image = np.clip(output_image, 0, 1)\n",
    "#         clean_image = np.clip(clean_image, 0, 1)\n",
    "\n",
    "#         # Calculate PSNR (assuming images are in [0, 1])\n",
    "#         psnr_value = psnr(clean_image, output_image, data_range=1)\n",
    "#         psnr_values.append(psnr_value)\n",
    "\n",
    "#         # Calculate SSIM (assuming images are in [0, 1])\n",
    "#         ssim_value = ssim(clean_image, output_image, data_range=1, channel_axis=-1)\n",
    "#         ssim_values.append(ssim_value)\n",
    "\n",
    "#     # Return mean PSNR and SSIM across the batch\n",
    "#     return np.mean(psnr_values), np.mean(ssim_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: iterate through the DataLoader\n",
    "for images, labels in train_loader:\n",
    "    print(f\"Batch size: {images.size(0)}, Clean image shape: {images.size()}, Noisy image shape: {labels.size()}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs\n",
    "num_epochs = 50\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_history = []  # To store training loss for each epoch\n",
    "val_loss_history = []    # To store validation loss for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training function\n",
    "def train_drunet(drunet, train_loader, val_loader, num_epochs, learning_rate):\n",
    "    drunet.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(drunet.parameters(), lr=learning_rate)\n",
    "    \n",
    "    best_val_loss = float('inf')  # Initialize to infinity\n",
    "    best_epoch = 0  # To keep track of which epoch had the best model\n",
    "    early_stop_patience = 10\n",
    "    patience_counter = 0\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # --- TRAINING PHASE ---\n",
    "        drunet.train() # Set the model to training mode\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        \n",
    "        for noisy_images, clean_images in train_loader:\n",
    "            noisy_images = noisy_images.to(device)\n",
    "            clean_images = clean_images.to(device)\n",
    "            \n",
    "            # Check shapes\n",
    "            # print(f'Noisy images shape: {noisy_images.shape}')  # Should be (B, 3, 224, 224) or (B, C, H, W)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = drunet(noisy_images)\n",
    "            loss = criterion(outputs, clean_images)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * noisy_images.size(0)\n",
    "            \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        train_loss_history.append(avg_train_loss)  # Store train loss\n",
    "        \n",
    "        # Validation phase\n",
    "        drunet.eval() # Set the model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        total_psnr = 0.0\n",
    "        total_ssim = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for noisy_images, clean_images in val_loader:\n",
    "                noisy_images = noisy_images.to(device)\n",
    "                clean_images = clean_images.to(device)\n",
    "\n",
    "                # Forward pass: Get model predictions\n",
    "                outputs = drunet(noisy_images)\n",
    "                loss = criterion(outputs, clean_images)\n",
    "                \n",
    "                # Accumulate the validation loss\n",
    "                val_loss += loss.item()*noisy_images.size(0)\n",
    "                \n",
    "                # print(f\"Outputs shape: {outputs.shape}, Clean images shape: {clean_images.shape}\")\n",
    "                psnr_value, ssim_value = calculate_metrics(outputs, clean_images)\n",
    "                total_psnr += psnr_value\n",
    "                total_ssim += ssim_value\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_loss_history.append(avg_val_loss)  # Store validation loss\n",
    "        \n",
    "        avg_psnr = total_psnr / len(val_loader)\n",
    "        avg_ssim = total_ssim / len(val_loader)\n",
    "        \n",
    "        # Check if this is the best validation loss so far\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_epoch = epoch + 1\n",
    "            patience_counter = 0  # Reset patience counter if validation improves\n",
    "            torch.save(drunet.state_dict(), 'Saturation_Brdnet.pt')  # Save the model\n",
    "            print(f'Saved best model at epoch {best_epoch} with validation loss: {best_val_loss:.4f}')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        # Early stopping condition\n",
    "        if patience_counter >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch+1} due to no improvement in validation loss for {early_stop_patience} consecutive epochs.')\n",
    "            break\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}')\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Average PSNR: {avg_psnr:.4f}, Average SSIM: {avg_ssim:.4f}')\n",
    "        print(f'Time taken: {time.time() - start_time:.2f} seconds\\n')\n",
    "    print(f\"Training complete. Best model saved at epoch {best_epoch} with validation loss: {best_val_loss:.4f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_test(drunet, test_loader):\n",
    "    # Load the best model saved during training\n",
    "    drunet.load_state_dict(torch.load('Saturation_Brdnet.pt'))\n",
    "    drunet.eval()\n",
    "    test_loss = 0\n",
    "    total_psnr = 0\n",
    "    total_ssim = 0\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for noisy_images, clean_images in test_loader:  # Use test_loader here\n",
    "            noisy_images = noisy_images.to(device)\n",
    "            clean_images = clean_images.to(device)\n",
    "\n",
    "            outputs = drunet(noisy_images)\n",
    "            loss = criterion(outputs, clean_images)\n",
    "            test_loss += loss.item()*noisy_images.size(0)\n",
    "            # print(test_loss)\n",
    "\n",
    "            psnr_value, ssim_value = calculate_metrics(outputs, clean_images)\n",
    "            total_psnr += psnr_value\n",
    "            total_ssim += ssim_value\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    avg_psnr = total_psnr / len(test_loader)\n",
    "    avg_ssim = total_ssim / len(test_loader)\n",
    "\n",
    "    print(f'Test Loss: {avg_test_loss:.4f}, Average PSNR: {avg_psnr:.4f}, Average SSIM: {avg_ssim:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training the model\n",
    "train_drunet(brd_model, train_loader, val_loader, num_epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot the Loss Curves ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss_history, label='Training Loss')\n",
    "plt.plot(val_loss_history, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_on_test(brd_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "brd_model.load_state_dict(torch.load('Saturation_Brdnet.pt'))\n",
    "brd_model.to(device)\n",
    "# Set the model to evaluation mode\n",
    "brd_model.eval()\n",
    "\n",
    "# Get a few images from the test set\n",
    "with torch.no_grad():\n",
    "    # Get a single batch from the test loader\n",
    "    noisy_images, clean_images = next(iter(test_loader))\n",
    "    \n",
    "    # Move to the appropriate device\n",
    "    noisy_images = noisy_images.to(device)\n",
    "    clean_images = clean_images.to(device)\n",
    "\n",
    "    # Pass the noisy images through the drunet\n",
    "    denoised_images = brd_model(noisy_images)\n",
    "\n",
    "# Convert to numpy for visualization\n",
    "noisy_images_np = noisy_images.cpu().numpy()\n",
    "clean_images_np = clean_images.cpu().numpy()\n",
    "denoised_images_np = denoised_images.cpu().numpy()\n",
    "\n",
    "# Function to visualize images\n",
    "def plot_images(noisy, clean, denoised, num_images=4):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i in range(num_images):\n",
    "        # Noisy image\n",
    "        plt.subplot(3, num_images, i + 1)\n",
    "        plt.imshow(noisy[i].transpose(1, 2, 0))\n",
    "        plt.title(\"Noisy\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # Clean image\n",
    "        plt.subplot(3, num_images, i + 1 + num_images)\n",
    "        plt.imshow(clean[i].transpose(1, 2, 0))\n",
    "        plt.title(\"Clean\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # Denoised image\n",
    "        plt.subplot(3, num_images, i + 1 + 2 * num_images)\n",
    "        plt.imshow(denoised[i].transpose(1, 2, 0))\n",
    "        plt.title(\"Denoised\")\n",
    "        plt.axis(\"off\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Plot the images\n",
    "plot_images(noisy_images_np, clean_images_np, denoised_images_np, num_images=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'denoising_dataset/Brightness/L5/COCO_val2014_000000000074.jpg'\n",
    "noisy_image = Image.open(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations to convert the image to tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts the image to [0, 1] range and converts to tensor\n",
    "    transforms.Resize((224, 224)),  # Resizes the image if necessary\n",
    "])\n",
    "\n",
    "# Apply transformation and add batch dimension (batch size 1)\n",
    "noisy_image_tensor = transform(noisy_image).unsqueeze(0)  # Shape: [1, channels, height, width]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the model is in evaluation mode\n",
    "brd_model.eval()\n",
    "\n",
    "# If using a GPU, move the image tensor to the GPU\n",
    "noisy_image_tensor = noisy_image_tensor.to('cuda')  # Use 'cpu' if no GPU is available\n",
    "\n",
    "# Perform inference without computing gradients\n",
    "with torch.no_grad():\n",
    "    denoised_image = brd_model(noisy_image_tensor)\n",
    "\n",
    "# The output denoised_image will be a tensor, shape: [1, channels, height, width]\n",
    "print(denoised_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Move the output back to CPU if using GPU\n",
    "denoised_image = denoised_image.cpu()\n",
    "noisy_image_tensor = noisy_image_tensor.cpu()\n",
    "\n",
    "# Remove batch dimensions for both images\n",
    "noisy_image = noisy_image_tensor.squeeze(0)  # Shape: [channels, height, width]\n",
    "denoised_image = denoised_image.squeeze(0)  # Shape: [channels, height, width]\n",
    "\n",
    "# Convert both tensors to PIL images\n",
    "noisy_image = T.ToPILImage()(noisy_image)\n",
    "denoised_image = T.ToPILImage()(denoised_image)\n",
    "\n",
    "\n",
    "# Plot both images side by side\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Display noisy image\n",
    "axs[0].imshow(noisy_image)\n",
    "axs[0].axis('off')\n",
    "axs[0].set_title('Noisy Image')\n",
    "\n",
    "# Display denoised image\n",
    "axs[1].imshow(denoised_image)\n",
    "axs[1].axis('off')\n",
    "axs[1].set_title('Denoised Image')\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'denoising_dataset/Brightness/L4/COCO_val2014_000000000074.jpg'\n",
    "noisy_image = Image.open(image_path)\n",
    "\n",
    "# Define transformations to convert the image to tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts the image to [0, 1] range and converts to tensor\n",
    "    transforms.Resize((224, 224)),  # Resizes the image if necessary\n",
    "])\n",
    "\n",
    "# Apply transformation and add batch dimension (batch size 1)\n",
    "noisy_image_tensor = transform(noisy_image).unsqueeze(0)  # Shape: [1, channels, height, width]\n",
    "\n",
    "# Make sure the model is in evaluation mode\n",
    "brd_model.eval()\n",
    "\n",
    "# If using a GPU, move the image tensor to the GPU\n",
    "noisy_image_tensor = noisy_image_tensor.to('cuda')  # Use 'cpu' if no GPU is available\n",
    "\n",
    "# Perform inference without computing gradients\n",
    "with torch.no_grad():\n",
    "    denoised_image = brd_model(noisy_image_tensor)\n",
    "\n",
    "# The output denoised_image will be a tensor, shape: [1, channels, height, width]\n",
    "print(denoised_image.shape)\n",
    "\n",
    "denoised_image = denoised_image.cpu()\n",
    "noisy_image_tensor = noisy_image_tensor.cpu()\n",
    "\n",
    "# Remove batch dimensions for both images\n",
    "noisy_image = noisy_image_tensor.squeeze(0)  # Shape: [channels, height, width]\n",
    "denoised_image = denoised_image.squeeze(0)  # Shape: [channels, height, width]\n",
    "\n",
    "# Convert both tensors to PIL images\n",
    "noisy_image = T.ToPILImage()(noisy_image)\n",
    "denoised_image = T.ToPILImage()(denoised_image)\n",
    "\n",
    "\n",
    "# Plot both images side by side\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Display noisy image\n",
    "axs[0].imshow(noisy_image)\n",
    "axs[0].axis('off')\n",
    "axs[0].set_title('Noisy Image')\n",
    "\n",
    "# Display denoised image\n",
    "axs[1].imshow(denoised_image)\n",
    "axs[1].axis('off')\n",
    "axs[1].set_title('Denoised Image')\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'denoising_dataset/Spatter/L5/COCO_val2014_000000000073.jpg'\n",
    "noisy_image = Image.open(image_path)\n",
    "\n",
    "# Define transformations to convert the image to tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts the image to [0, 1] range and converts to tensor\n",
    "    transforms.Resize((224, 224)),  # Resizes the image if necessary\n",
    "])\n",
    "\n",
    "# Apply transformation and add batch dimension (batch size 1)\n",
    "noisy_image_tensor = transform(noisy_image).unsqueeze(0)  # Shape: [1, channels, height, width]\n",
    "\n",
    "# Make sure the model is in evaluation mode\n",
    "brd_model.eval()\n",
    "\n",
    "# If using a GPU, move the image tensor to the GPU\n",
    "noisy_image_tensor = noisy_image_tensor.to('cuda')  # Use 'cpu' if no GPU is available\n",
    "\n",
    "# Perform inference without computing gradients\n",
    "with torch.no_grad():\n",
    "    denoised_image = brd_model(noisy_image_tensor)\n",
    "\n",
    "# The output denoised_image will be a tensor, shape: [1, channels, height, width]\n",
    "print(denoised_image.shape)\n",
    "\n",
    "denoised_image = denoised_image.cpu()\n",
    "noisy_image_tensor = noisy_image_tensor.cpu()\n",
    "\n",
    "# Remove batch dimensions for both images\n",
    "noisy_image = noisy_image_tensor.squeeze(0)  # Shape: [channels, height, width]\n",
    "denoised_image = denoised_image.squeeze(0)  # Shape: [channels, height, width]\n",
    "\n",
    "# Convert both tensors to PIL images\n",
    "noisy_image = T.ToPILImage()(noisy_image)\n",
    "denoised_image = T.ToPILImage()(denoised_image)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plot both images side by side\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Display noisy image\n",
    "axs[0].imshow(noisy_image)\n",
    "axs[0].axis('off')\n",
    "axs[0].set_title('Noisy Image')\n",
    "\n",
    "# Display denoised image\n",
    "axs[1].imshow(denoised_image)\n",
    "axs[1].axis('off')\n",
    "axs[1].set_title('Denoised Image')\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'denoising_dataset/Spatter/L5/COCO_val2014_000000000139.jpg'\n",
    "noisy_image = Image.open(image_path)\n",
    "\n",
    "# Define transformations to convert the image to tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts the image to [0, 1] range and converts to tensor\n",
    "    transforms.Resize((224, 224)),  # Resizes the image if necessary\n",
    "])\n",
    "\n",
    "# Apply transformation and add batch dimension (batch size 1)\n",
    "noisy_image_tensor = transform(noisy_image).unsqueeze(0)  # Shape: [1, channels, height, width]\n",
    "\n",
    "# Make sure the model is in evaluation mode\n",
    "brd_model.eval()\n",
    "\n",
    "# If using a GPU, move the image tensor to the GPU\n",
    "noisy_image_tensor = noisy_image_tensor.to('cuda')  # Use 'cpu' if no GPU is available\n",
    "\n",
    "# Perform inference without computing gradients\n",
    "with torch.no_grad():\n",
    "    denoised_image = brd_model(noisy_image_tensor)\n",
    "\n",
    "# The output denoised_image will be a tensor, shape: [1, channels, height, width]\n",
    "print(denoised_image.shape)\n",
    "\n",
    "denoised_image = denoised_image.cpu()\n",
    "noisy_image_tensor = noisy_image_tensor.cpu()\n",
    "\n",
    "# Remove batch dimensions for both images\n",
    "noisy_image = noisy_image_tensor.squeeze(0)  # Shape: [channels, height, width]\n",
    "denoised_image = denoised_image.squeeze(0)  # Shape: [channels, height, width]\n",
    "\n",
    "# Convert both tensors to PIL images\n",
    "noisy_image = T.ToPILImage()(noisy_image)\n",
    "denoised_image = T.ToPILImage()(denoised_image)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plot both images side by side\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Display noisy image\n",
    "axs[0].imshow(noisy_image)\n",
    "axs[0].axis('off')\n",
    "axs[0].set_title('Noisy Image')\n",
    "\n",
    "# Display denoised image\n",
    "axs[1].imshow(denoised_image)\n",
    "axs[1].axis('off')\n",
    "axs[1].set_title('Denoised Image')\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'denoising_dataset/Spatter/L5/COCO_val2014_000000000241.jpg'\n",
    "noisy_image = Image.open(image_path)\n",
    "\n",
    "# Define transformations to convert the image to tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts the image to [0, 1] range and converts to tensor\n",
    "    transforms.Resize((224, 224)),  # Resizes the image if necessary\n",
    "])\n",
    "\n",
    "# Apply transformation and add batch dimension (batch size 1)\n",
    "noisy_image_tensor = transform(noisy_image).unsqueeze(0)  # Shape: [1, channels, height, width]\n",
    "\n",
    "# Make sure the model is in evaluation mode\n",
    "brd_model.eval()\n",
    "\n",
    "# If using a GPU, move the image tensor to the GPU\n",
    "noisy_image_tensor = noisy_image_tensor.to('cuda')  # Use 'cpu' if no GPU is available\n",
    "\n",
    "# Perform inference without computing gradients\n",
    "with torch.no_grad():\n",
    "    denoised_image = brd_model(noisy_image_tensor)\n",
    "\n",
    "# The output denoised_image will be a tensor, shape: [1, channels, height, width]\n",
    "print(denoised_image.shape)\n",
    "\n",
    "denoised_image = denoised_image.cpu()\n",
    "noisy_image_tensor = noisy_image_tensor.cpu()\n",
    "\n",
    "# Remove batch dimensions for both images\n",
    "noisy_image = noisy_image_tensor.squeeze(0)  # Shape: [channels, height, width]\n",
    "denoised_image = denoised_image.squeeze(0)  # Shape: [channels, height, width]\n",
    "\n",
    "# Convert both tensors to PIL images\n",
    "noisy_image = T.ToPILImage()(noisy_image)\n",
    "denoised_image = T.ToPILImage()(denoised_image)\n",
    "\n",
    "\n",
    "# Plot both images side by side\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Display noisy image\n",
    "axs[0].imshow(noisy_image)\n",
    "axs[0].axis('off')\n",
    "axs[0].set_title('Noisy Image')\n",
    "\n",
    "# Display denoised image\n",
    "axs[1].imshow(denoised_image)\n",
    "axs[1].axis('off')\n",
    "axs[1].set_title('Denoised Image')\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
