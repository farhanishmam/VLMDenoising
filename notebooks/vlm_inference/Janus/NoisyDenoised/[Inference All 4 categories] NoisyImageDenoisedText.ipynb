{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/deepseek-ai/Janus\n",
    "# !cd Janus\n",
    "\n",
    "# !pip install torch==2.0.1 transformers>=4.38.2 timm>=0.9.16 accelerate sentencepiece attrdict einops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd Janus/janus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install janus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip show janus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import janus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append(\"Janus/janus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip show janus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install Janus/janus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"User\",\n",
    "        \"content\": \"<image_placeholder>\\nConvert the formula into latex code.\",\n",
    "        \"images\": [\"images/equation.png\"],\n",
    "    },\n",
    "    {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    \n",
    "]\n",
    "\n",
    "# load images and prepare for inputs\n",
    "pil_images = load_pil_images(conversation)\n",
    "prepare_inputs = vl_chat_processor(\n",
    "    conversations=conversation, images=pil_images, force_batchify=True\n",
    ").to(vl_gpt.device)\n",
    "\n",
    "# # run image encoder to get the image embeddings\n",
    "inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "# # run the model to get the response\n",
    "outputs = vl_gpt.language_model.generate(\n",
    "    inputs_embeds=inputs_embeds,\n",
    "    attention_mask=prepare_inputs.attention_mask,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "print(f\"{prepare_inputs['sft_format'][0]}\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------- Count -------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available. Device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Using device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Current device index: {torch.cuda.current_device()}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brightness-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"count\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"count\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Brightness/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about count. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrast-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"count\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"count\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Contrast/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about count. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defocus-blur-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"count\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"count\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Defocus-blur/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about count. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"count\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"count\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Elastic/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about count. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fog-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"count\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"count\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Fog/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about count. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frost-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"count\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"count\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Frost/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about count. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian-noise-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"count\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"count\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Gaussian-noise/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about count. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impulse-Noise-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"count\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"count\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Impulse-noise/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about count. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JPEG-compresion-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"count\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"count\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/JPEG-compression/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about count. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pixelate-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"count\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"count\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Pixelate/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about count. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rain-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"count\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"count\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Rain/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about count. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saturation-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"count\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"count\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Saturation/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about count. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shot-noise-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"count\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"count\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Shot-noise/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about count. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snow-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"count\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"count\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Snow/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about count. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatter-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"count\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"count\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Spatter/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about count. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speckle-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"count\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"count\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Speckle-noise/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about count. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zoom-blur-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"count\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"count\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Zoom-Blur/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about count. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion-blur-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"count\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"count\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Motion-blur/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about count. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------- Order -------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available. Device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Using device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Current device index: {torch.cuda.current_device()}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brightness-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"order\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"order\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Brightness/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about order. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrast-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"order\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"order\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Contrast/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about order. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defocus-blur-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"order\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"order\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Defocus-blur/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about order. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"order\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"order\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Elastic/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about order. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fog-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"order\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"order\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Fog/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about order. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frost-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"order\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"order\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Frost/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about order. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian-noise-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"order\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"order\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Gaussian-noise/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about order. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impulse-Noise-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"order\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"order\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Impulse-noise/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about order. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JPEG-compresion-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"order\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"order\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/JPEG-compression/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about order. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pixelate-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"order\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"order\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Pixelate/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about order. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rain-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"order\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"order\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Rain/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about order. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saturation-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"order\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"order\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Saturation/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about order. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shot-noise-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"order\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"order\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Shot-noise/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about order. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snow-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"order\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"order\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Snow/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about order. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatter-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"order\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"order\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Spatter/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about order. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speckle-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"order\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"order\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Speckle-noise/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about order. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zoom-blur-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"order\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"order\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Zoom-Blur/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about order. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion-blur-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"order\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"order\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Motion-blur/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about order. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------- Trick -------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available. Device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Using device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Current device index: {torch.cuda.current_device()}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brightness-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"trick\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"trick\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Brightness/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about trick. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrast-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"trick\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"trick\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Contrast/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about trick. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defocus-blur-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"trick\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"trick\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Defocus-blur/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about trick. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"trick\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"trick\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Elastic/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about trick. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fog-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"trick\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"trick\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Fog/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about trick. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frost-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"trick\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"trick\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Frost/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about trick. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian-noise-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"trick\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"trick\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Gaussian-noise/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about trick. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impulse-Noise-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"trick\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"trick\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Impulse-noise/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about trick. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JPEG-compresion-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"trick\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"trick\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/JPEG-compression/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about trick. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pixelate-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"trick\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"trick\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Pixelate/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about trick. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rain-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"trick\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"trick\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Rain/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about trick. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saturation-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"trick\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"trick\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Saturation/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about trick. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shot-noise-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"trick\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"trick\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Shot-noise/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about trick. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snow-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"trick\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"trick\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Snow/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about trick. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatter-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"trick\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"trick\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Spatter/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about trick. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speckle-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"trick\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"trick\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Speckle-noise/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about trick. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zoom-blur-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"trick\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"trick\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Zoom-Blur/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about trick. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion-blur-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"trick\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"trick\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Motion-blur/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about trick. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------- VCR -------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available. Device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Using device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Current device index: {torch.cuda.current_device()}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brightness-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"vcr\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"vcr\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Brightness/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about vcr. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrast-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"vcr\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"vcr\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Contrast/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about vcr. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defocus-blur-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"vcr\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"vcr\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Defocus-blur/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about vcr. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"vcr\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"vcr\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Elastic/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about vcr. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fog-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"vcr\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"vcr\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Fog/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about vcr. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frost-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"vcr\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"vcr\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Frost/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about vcr. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian-noise-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"vcr\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"vcr\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Gaussian-noise/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about vcr. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impulse-Noise-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"vcr\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"vcr\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Impulse-noise/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about vcr. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JPEG-compresion-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"vcr\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"vcr\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/JPEG-compression/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about vcr. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pixelate-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"vcr\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"vcr\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Pixelate/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about vcr. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rain-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"vcr\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"vcr\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Rain/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about vcr. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saturation-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"vcr\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"vcr\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Saturation/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about vcr. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shot-noise-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"vcr\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"vcr\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Shot-noise/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about vcr. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snow-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"vcr\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"vcr\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Snow/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about vcr. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatter-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"vcr\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"vcr\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Spatter/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about vcr. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speckle-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"vcr\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"vcr\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Speckle-noise/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about vcr. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zoom-blur-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"vcr\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"vcr\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Zoom-Blur/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about vcr. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion-blur-Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# specify the path to the model\n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)\n",
    "tokenizer = vl_chat_processor.tokenizer\n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, trust_remote_code=True\n",
    ")\n",
    "vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Noisy-Denoised_QuestionPairs[new].csv\")\n",
    "\n",
    "# Initialize counters for accuracy per noise type\n",
    "noise_type_accuracy = defaultdict(lambda: {\"correct\": 0, \"total\": 0})\n",
    "\n",
    "# Iterate over rows where category is \"vcr\"\n",
    "for index, row in data.iterrows():\n",
    "    if row[\"category\"] != \"vcr\":\n",
    "        continue  # Skip non-count categories\n",
    "\n",
    "    # Load the corresponding image\n",
    "    image_path = f\"Noisy DARE TEST/Motion-blur/{row['path']}\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {image_path}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"User\",\n",
    "            \"content\": \"<image_placeholder>\\nThe following are multiple choice questions about vcr. You should directly answer the question by choosing the correct option given the image and the question. Give only the letter indicating the correct answer e.g. 'A'\\n\"\n",
    "            f\"Question: {row['denoised_question']}\\n\"\n",
    "            \"Options:\\n\"\n",
    "            f\"A. {row['A']}\\n\"\n",
    "            f\"B. {row['B']}\\n\"\n",
    "            f\"C. {row['C']}\\n\"\n",
    "            f\"D. {row['D']}\\n\"\n",
    "            \"Answer:\",\n",
    "            \"images\": [image_path],\n",
    "        },\n",
    "        {\"role\": \"Assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    # load images and prepare for inputs\n",
    "    pil_images = load_pil_images(conversation)\n",
    "    prepare_inputs = vl_chat_processor(\n",
    "        conversations=conversation, images=pil_images, force_batchify=True\n",
    "    ).to(vl_gpt.device)\n",
    "\n",
    "    # run image encoder to get the image embeddings\n",
    "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
    "\n",
    "    # run the model to get the response\n",
    "    outputs = vl_gpt.language_model.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=prepare_inputs.attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    predicted_answer = answer.split()[-1]  # Get the last word (e.g., \"A\", \"B\", \"C\", or \"D\")\n",
    "\n",
    "    # Get the noise type\n",
    "    noise_type = row[\"modified_question_function_name\"]\n",
    "\n",
    "    # Compare with the actual answer and update counters\n",
    "    if predicted_answer == row[\"answer\"][2]:\n",
    "        noise_type_accuracy[noise_type][\"correct\"] += 1\n",
    "    noise_type_accuracy[noise_type][\"total\"] += 1\n",
    "\n",
    "    # Print the prediction\n",
    "    print(f\"Question ID {row['id']} - Predicted Answer: {predicted_answer} - Actual Answer: {row['answer']} - Noise Type: {noise_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for overall accuracy\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# Calculate and print accuracy per noise type\n",
    "for noise_type, counts in noise_type_accuracy.items():\n",
    "    correct = counts[\"correct\"]\n",
    "    total = counts[\"total\"]\n",
    "    print(f\"Noise Type : {noise_type}, Correct Predictions: {correct}, Total: {total}\")\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"Accuracy for noise type '{noise_type}': {accuracy:.2f}%\")\n",
    "    \n",
    "    # Update overall counters\n",
    "    total_correct += correct\n",
    "    total_predictions += total\n",
    "\n",
    "# Calculate overall accuracy\n",
    "print(f\"Total Correct: {total_correct}, Total Predictions: {total_predictions}\")\n",
    "overall_accuracy = (total_correct / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "print(f\"\\nOverall Accuracy: {overall_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
