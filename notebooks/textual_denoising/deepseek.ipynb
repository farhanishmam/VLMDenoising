{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import GPUtil\n",
    "import pynvml\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "# -------------------------\n",
    "# Load model + tokenizer\n",
    "# -------------------------\n",
    "model_name = \"deepseek-ai/deepseek-llm-7b-chat\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda\"\n",
    ")\n",
    "model.generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "\n",
    "# -------------------------\n",
    "# Init NVML for GPU metrics\n",
    "# -------------------------\n",
    "pynvml.nvmlInit()\n",
    "gpu_handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "def get_gpu_utilization():\n",
    "    util = pynvml.nvmlDeviceGetUtilizationRates(gpu_handle)\n",
    "    mem = pynvml.nvmlDeviceGetMemoryInfo(gpu_handle)\n",
    "    return util.gpu, util.memory, mem.used / 1024**2  # % compute, % mem, MB\n",
    "\n",
    "# -------------------------\n",
    "# Load dataset\n",
    "# -------------------------\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'Noisy-Denoised_QuestionPairs[new].csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop the 'denoised_question' column\n",
    "df.drop(columns=['denoised_question'], inplace=True)\n",
    "\n",
    "# Save the updated CSV file\n",
    "df.to_csv('Noisy-Denoised_QuestionPairs[deepseek].csv', index=False)\n",
    "\n",
    "print(\"Column 'denoised_question' has been dropped and the updated file is saved as 'Noisy-Denoised_QuestionPairs[deepseek].csv'.\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Denoising + profiling\n",
    "# -------------------------\n",
    "def denoise_question(question):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \n",
    "            f\"\"\"You are an expert at denoising text. Your task is to provide the denoised version of a given noisy text or question. Follow these instructions:\n",
    "\n",
    "1. Return only the denoised version of the text or question.\n",
    "2. Do not provide explanations or additional words.\n",
    "3. Do not answer the question or alter its intent.\n",
    "4. Maintain the question format if the input is a question.\n",
    "5. Avoid presenting the answer in assertive form.\n",
    "\\n '{question}' \"\"\"}\n",
    "    ]\n",
    "    input_tensor = tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # GPU/CPU usage before\n",
    "    cpu_before = psutil.cpu_percent(interval=None)\n",
    "    gpu_util_before, gpu_mem_util_before, gpu_mem_used_before = get_gpu_utilization()\n",
    "\n",
    "    start = time.time()\n",
    "    outputs = model.generate(input_tensor.to(model.device), max_new_tokens=100)\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "\n",
    "    # GPU/CPU usage after\n",
    "    cpu_after = psutil.cpu_percent(interval=None)\n",
    "    gpu_util_after, gpu_mem_util_after, gpu_mem_used_after = get_gpu_utilization()\n",
    "\n",
    "    # Tokens\n",
    "    input_tokens = input_tensor.shape[1]\n",
    "    output_tokens = outputs.shape[1] - input_tokens\n",
    "    total_tokens = input_tokens + output_tokens\n",
    "\n",
    "    result = tokenizer.decode(outputs[0][input_tokens:], skip_special_tokens=True)\n",
    "\n",
    "    return {\n",
    "        \"denoised\": result.strip(),\n",
    "        \"time_sec\": end - start,\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens\": output_tokens,\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"cpu_usage_%\": (cpu_before + cpu_after) / 2,\n",
    "        \"gpu_util_%\": (gpu_util_before + gpu_util_after) / 2,\n",
    "        \"gpu_mem_util_%\": (gpu_mem_util_before + gpu_mem_util_after) / 2,\n",
    "        \"gpu_mem_MB\": gpu_mem_used_after\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Run profiling\n",
    "# -------------------------\n",
    "timings = []\n",
    "denoised_questions = []\n",
    "total_time = 0\n",
    "total_tokens = 0\n",
    "\n",
    "for modified in tqdm(df['modified_question'], desc=\"Profiling Denoising\"):\n",
    "    metrics = denoise_question(modified)\n",
    "    denoised_questions.append(metrics[\"denoised\"])\n",
    "    timings.append({\"question\": modified, **metrics})\n",
    "    total_time += metrics[\"time_sec\"]\n",
    "    total_tokens += metrics[\"total_tokens\"]\n",
    "\n",
    "# -------------------------\n",
    "# Summary stats\n",
    "# -------------------------\n",
    "n_samples = len(df)\n",
    "avg_time = total_time / n_samples\n",
    "avg_tokens = total_tokens / n_samples\n",
    "tokens_per_sec = total_tokens / total_time\n",
    "\n",
    "# FLOPs estimate: ~2 × (#params) × tokens\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "avg_flops = 2 * n_params * avg_tokens\n",
    "\n",
    "# Cost estimate: assume $1.50/hr for an A100 (adjust if needed)\n",
    "gpu_hourly_price = 1.50\n",
    "gpu_hours = total_time / 3600\n",
    "gpu_cost = gpu_hourly_price * gpu_hours\n",
    "\n",
    "print(\"\\n===== Inference Cost Report =====\")\n",
    "print(f\"Samples processed      : {n_samples}\")\n",
    "print(f\"Total time (s)         : {total_time:.2f}\")\n",
    "print(f\"Avg time per sample    : {avg_time:.3f} s\")\n",
    "print(f\"Avg tokens per sample  : {avg_tokens:.1f}\")\n",
    "print(f\"Tokens/sec             : {tokens_per_sec:.1f}\")\n",
    "print(f\"Model parameters       : {n_params/1e9:.2f} B\")\n",
    "print(f\"Est. FLOPs/sample      : {avg_flops/1e12:.2f} TFLOPs\")\n",
    "print(f\"GPU memory usage (MB)  : {timings[0]['gpu_mem_MB']:.1f}\")\n",
    "print(f\"GPU utilization (avg%) : {sum(t['gpu_util_%'] for t in timings)/n_samples:.1f}\")\n",
    "print(f\"CPU utilization (avg%) : {sum(t['cpu_usage_%'] for t in timings)/n_samples:.1f}\")\n",
    "print(f\"Est. GPU runtime (h)   : {gpu_hours:.3f}\")\n",
    "print(f\"Est. GPU cost ($)      : {gpu_cost:.2f}\")\n",
    "\n",
    "# -------------------------\n",
    "# Save results\n",
    "# -------------------------\n",
    "profiled_df = pd.DataFrame(timings)\n",
    "profiled_df.to_csv(\"profiling_results_detailed.csv\", index=False)\n",
    "\n",
    "df[\"denoised_question\"] = denoised_questions\n",
    "df.to_csv(\"Noisy-Denoised_QuestionPairs[deepseek]_profiled.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
